---
title: "Results"
author: ''
date: " "
always_allow_html: yes
output:
  html_document:
    theme: flatly
  pdf_document:
    toc: yes
  word_document: default
---




Last update: `r Sys.time()`


```{r set-global-options, echo = FALSE}
knitr::opts_chunk$set(eval = TRUE, 
                      echo = FALSE, 
                      message=FALSE,
                      warning = FALSE,
                      cache = FALSE,
                      include = TRUE,
                      collapse = FALSE,
                      dependson = NULL,
                      engine = "R", #  Chunks will always have R code, unless noted
                      error = TRUE,
                      fig.path="Figures/",  #  Set the figure options
                      fig.align = "left",
                      fig.width = 8,
                      fig.height = 4)
```




```{r silent-packages, echo = FALSE, eval = TRUE, message=FALSE, include = FALSE}

#libraries
library(sjPlot)
library(sjstats)
library(tidyverse)
library(car)
library(ggthemes)
library(kableExtra)
library(ggpubr)
```

```{r}

#some settings
set_theme(
  base=theme_blank(),
  geom.outline.size = 0.01,
  geom.outline.color = "white", 
  geom.label.size = 3,
  geom.label.color = "grey50")


predictors.labs <- c(
  `(Intercept)` = "Intercept",
  advice.algorithm = "Algorithm",
  moroccan = "Moroccan teacher",
  `moroccan:advice.algorithm` = "Moroccan teacher x Algorithm",
  `scale(perceived.performance.item1, scale = F)` = 'Item1:\n"Algorithms take into account more information..."',
  `scale(perceived.performance.item2, scale = F)` = 'Item2:\n"Algorithms make better judgments..."',
  `scale(perceived.performance.item3, scale = F)` = 'Item3:\n"...algorithms make fairer judgments..."',
  `scale(female, scale = F)` = "Female",
  `scale(age, scale = F)`  = "Age",
  `scale(income, scale = F)` = "Income"
)

```


```{r}

#Importing data
#load("C:/SAAR/UNIVERSITY/R/NL.SCHOOLS/.RData")
source("C:/SAAR/UNIVERSITY/R/NL.SCHOOLS/code/nl.schools.study1.dm_01.R")
source("C:/SAAR/UNIVERSITY/R/NL.SCHOOLS/code/nl.schools.study2.dm_01.R")
source("C:/SAAR/UNIVERSITY/R/NL.SCHOOLS/code/nl.schools.long_01.R")


```


<br>

```{r}
table.study.1 <- school.s1_01 %>% 
  group_by(advice) %>% 
  dplyr::summarise(n = n(),
            percent.follow.advice = (mean(follow.advice))*100)

chisq.test.study1 <- chisq.test(school.s1_01$follow.advice,school.s1_01$advice) 

table.study.1.robust <- school.s1_02 %>% 
  group_by(advice) %>% 
  summarise(n = n(),
            percent.follow.advice = (mean(follow.advice))*100)


chisq.test.study1.robust <- chisq.test(school.s1_02$follow.advice,school.s1_02$advice) 



table.study.2_2 <- school.s2_01.full %>% 
  group_by(advice) %>% 
  summarise(n = n(),
            percent.follow.advice = (mean(follow.advice))*100)

chisq.test.study2 <- chisq.test(school.s2_01.full$follow.advice,school.s2_01.full$advice) 

```

Based on our first hypothesis, we expect the probability of following the advice of the ILE score to be significantly higher among those assigned to the algorithmic advice, compared to those receiving a similar prediction by human consultants. However, in contrast with this expectation, in study 1, we find  not very small differences between these two groups (`r table.study.1 %>% filter(advice=="algorithm") %>% .$percent.follow.advice %>% round(1)`% in the algorithmic condition versus `r table.study.1 %>% filter(advice=="human") %>% .$percent.follow.advice %>% round(1)`%). Indeed, under both conditions, the vast majority of the participants choose to override the default of the ILE score, and instead to fire the teacher with the poorest qualitative evaluation.[^footnote_follow_qualitative] We further replicated these analyses with regard to the sample of Study 2 (*N*=`r school.s2_01.full %>% nrow()`). The differences are in the expected direction, yet they are still relatively small (`r table.study.2_2 %>% filter(advice=="algorithm") %>% .$percent.follow.advice %>% round(1)`% vs. `r table.study.2_2 %>% filter(advice=="human") %>% .$percent.follow.advice %>% round(1)`%, respectively). 

[^footnote_follow_qualitative]: `r (((school.s1_01 %>% filter(decision==3) %>% nrow())/(school.s1_01 %>% nrow()))*100) %>% round(0)`% of all participants selected the participants selected the teacher with the poorest HR evaluation.
 
We further analyzed the effect of our manipulation of algorithmic versus human advice via logistic regression analyses, presented in **Table 1**. In Models 1.1 and 1.2 we we regress our binary outcome variable on the advice manipulation among the two studies, separately, and thereafter in Model 1.3, we combined the two samples (*N*=`r school.all %>% nrow() %>% format(big.mark = ",")`). The combined sample is sufficiently powered to detect an effect size equivalent to a 36% increase in the probability of following the ILE score, an effect size which is generally considered a small-size effect (Chen, Cohen & Chen, 2010).[^footnote_power_h1] Finally, in Models 1.4 and 1.5, we further tested the robustness of these findings by restricting our combined sample to those who completed our manipulation checks as well as by controlling for demographics, with no major changes to the results. The effect of receiving an advice from an algorithm (compared with a human-expert) is statistically insignificant in all these analyses.

[^footnote_power_h1]: *Power*=0.8, *p*=0.05 (one-sided test).

<br>

**Table 1**

```{r}
tmod1 <- glm(
  teacher.fired ~ advice.algorithm,
  family = "binomial",
  data = school.s1_01.long %>%
    filter(ILE == 4)
)

tmod2 <- tmod1 %>% update(data = school.s2_01.full.long %>%
                            filter(ILE == 4))

tmod3 <- tmod1 %>% update(data = school.all.long %>%
                            filter(ILE == 4))

tmod4 <- tmod1 %>% update(data = school.all.long %>%
                            filter(ILE == 4,
                                   man.check.all == 1))

tmod5 <- tmod3 %>% update(.~.+
                            scale(female, scale = F) +
                            scale(age, scale = F) +
                            scale(income, scale = F))


tab_model(tmod1, tmod2, tmod3, tmod4,tmod5,
          show.stat = T,
          show.loglik = T,
          show.r2 = F,
          collapse.ci = T,
          emph.p = F,
          dv.labels = c("Study 1 (1.1)", "Study 2 (1.2)","Combined (1.3)", "Combined (robust samples) (1.4)","Combined (1.3)"),
          string.est = "OR (95% CI)",
          string.stat = "z",
          string.p = "p-value",
          pred.labels = predictors.labs,
          order.terms = c(2:5,1),
          title = "Outcome variable: following the advice (binary)")


#setting prediction of probabilities
#https://fromthebottomoftheheap.net/2017/05/01/glm-prediction-intervals-i/

ilink <- family(tmod4)$linkinv
pd.advice.algorithm <- data.frame(advice.algorithm = c(0,1))

pd.advice.algorithm <- pd.advice.algorithm %>% 
  cbind(predict(tmod4, pd.advice.algorithm,
                type = "link", se.fit = TRUE)[1:2]) %>% 
  transform(
    Fitted = ilink(fit),
    Upper = ilink(fit + (1.96 * se.fit)),
    Lower = ilink(fit - (1.96 * se.fit))
  )



```
*Note:* In all our regression tables, p-values refer to a two-sided test. Regression Tables produced via `sjPlot` R package (LÃ¼decke 2020).

<br>

We now turn to our second hypothesis. Based on that hypothesis, we expect that the probability of following the ILE algorithmic evaluation is higher among those participants who perceive computer algorithms as having greater performative capacities. The distributions of the three items used to account for the later, across the two studies, are presented in **Figure 2**. As explained above, these items were shown only to the algorithmic condition, after completing the experimental task and the manipulation checks. Overall, as shown by this figure, participants in our sample vary in their perceptions of the algorithms capacities. They tended to view algorithms as superior to humans regarding the ability to process large amounts of data (item 1, *Mean*=`r school.all$perceived.performance.item1 %>% mean(na.rm=T) %>% round(2)`, *SD*=`r school.all$perceived.performance.item1 %>% sd(na.rm=T) %>% round(2)`). On the other hand, many of them were still relatively skeptical about their potency to make "better decisions" (item 2, *Mean*=`r school.all$perceived.performance.item2 %>% mean(na.rm=T) %>% round(2)`, *SD*=`r school.all$perceived.performance.item2 %>% sd(na.rm=T) %>% round(2)`). The distribution of the third item that regards the fairness dimension is relatively symmetrical (*Mean*=`r school.all$perceived.performance.item3 %>% mean(na.rm=T) %>% round(2)`, *SD*=`r school.all$perceived.performance.item3 %>% sd(na.rm=T) %>% round(2)`).[^footnote_performance_distributions] 

[^footnote_performance_distributions]: These statistics refer to the combined sample. The mean scores of study 2 are slightly higher for all three items, yet these differences are small and marginally significant with regard to item 2 (`r ttest.item2 <- t.test(perceived.performance.item2~study,school.all)`*t*=`r ttest.item2[["statistic"]][["t"]] %>% abs() %>% round(3)`, *p*=`r ttest.item2[["p.value"]] %>% round(3)`), and insignificant for the other two items.    

<br>

```{r}

t1 <- school.all %>% 
  filter(perceived.performance.item1>=1) %>% 
  group_by(study) %>% 
  dplyr::summarise(median.item1 = median(perceived.performance.item1),
            median.item2 = median(perceived.performance.item2),
            median.item3 = median(perceived.performance.item3),
            mean.item1 = mean(perceived.performance.item1),
            mean.item2 = mean(perceived.performance.item2),
            mean.item3 = mean(perceived.performance.item3)
            )


#,
#

school.all <- school.all %>% 
  mutate(study.lab = study %>% recode("1='Study 1';2='Study 2'"))

p1 <- plot_grpfrq(school.all$perceived.performance.item1,
            school.all$study.lab,
        show.n = F,
        show.prc = F,
        legend.title="")+
  xlab("")+
  scale_y_continuous(limits = c(0,170),breaks = c(50,100,150))+
    ggtitle("Item 1",subtitle = '"Algorithms take into account\nmore information..."\n') + theme(plot.title.position = "plot")

p2 <- plot_grpfrq(school.all$perceived.performance.item2,
            school.all$study.lab,
        show.n = F,
        show.prc = F,
        legend.title="")+
    ggtitle("Item 2",subtitle = '"Algorithms make better\njudgments..."\n')+
    xlab("")+  
  scale_y_continuous(limits = c(0,170),breaks = c())+

  theme(plot.title.position = "plot")

p3 <- plot_grpfrq(school.all$perceived.performance.item3,
            school.all$study.lab,
                        ylim = c(0,170),
        show.n = F,
        show.prc = F,
        legend.title="")+
    xlab("")+
    scale_y_continuous(limits = c(0,170),breaks = c())+
    ggtitle("Item 3",subtitle = '"...algorithms make fairer\njudgments..."\n')  + theme(plot.title.position = "plot")              




ggarrange(p1,p2,p3,
          ncol=3,
          common.legend=T,
          legend = "bottom")

```

```{r}

tmod1 <- glm(
  follow.advice ~ scale(perceived.performance.item1,scale = F)+
    scale(perceived.performance.item2,scale = F)+
    scale(perceived.performance.item3,scale = F),
  family = binomial,
  data = school.s1_01 %>% filter(advice == "algorithm")
)

tmod2 <- tmod1 %>% update(data = school.s2_01.full %>% filter(advice == "algorithm"))

tmod3 <- tmod1 %>% update(data = school.all %>% filter(advice == "algorithm"))

tmod4 <- tmod3 %>% update(data = school.all %>% filter(advice == "algorithm",
                                                       man.check.all==1))

tmod5 <- tmod3 %>% update(.~.+
                            scale(female, scale = F) +
                            scale(age, scale = F) +
                            scale(income, scale = F))

tab.perceived.performance.item2 <- school.all %>%
  mutate(perceived.performance.item2.3levels = case_when(
    perceived.performance.item2<4 ~ "1-3",
    perceived.performance.item2==4 ~ "4",
    perceived.performance.item2>4 ~ "5-7",
  )) %>% 
  group_by(perceived.performance.item2.3levels) %>% 
  dplyr::summarise(n=n(),
                   percent.follow.ile = mean(follow.advice))


```


**Table 2** presents our logistic regression models with the three indicators which used to account for algorithms' capacities relative to humans. We find, consistently with our H2, that those who believe that algorithms can make better judgments than humans are significantly more likely to follow the algorithm (in face of the contradicting evidence of the qualitative evaluation). This effect replicated in our two studies (Models 2.1, 2.2). Descriptively, the compliance with the algorithmic ILE score is more than three times higher among who reported a score higher than 4 (the scale's middle point), compared with those who reported less than 4 (`r ((tab.perceived.performance.item2 %>% filter(perceived.performance.item2.3levels=="5-7") %>% select(percent.follow.ile) %>% max())*100) %>% round(0)`% versus `r ((tab.perceived.performance.item2 %>% filter(perceived.performance.item2.3levels=="1-3") %>% select(percent.follow.ile) %>% max())*100) %>% round(0)`% in the combined sample). We estimate that a one point increase in our 1-7 scale is associated with a 24--98% increase in participants' likelihood of following the ILE score (Model 2.3). This estimated effect increases when we restrict the sample to those who passed the manipulation checks (Model 2.4), and remains stable when we control for participants' gender, age and level of income (Model 2.5). In **Figure 3**, we graphically illustrate our prediction of Model 2.4. For comparison, we added to this plot the estimated probabilities (95% CIs) of following the human-expert based ILE score, which we extracted from the above model 1.4. 

Participants' agreement with the first item ("Algorithms take into account more information than humans") is positively associated with the outcome variable as expected (except for Model 2.1), yet these effects are small and far from statistical significance. Our third item ("In judgments that concern other people, algorithms make fairer judgments than humans") is actually found to be negatively correlated with adherence to algorithmic advice in both studies, yet these effects are not sufficiently statistically significant and reliable. 



<br>


**Table 2**

```{r}


tab_model(tmod1, tmod2, tmod3, tmod4, tmod5,
          show.stat = T,
          show.loglik = T,
          show.r2 = F,
          collapse.ci = T,
          emph.p = F,
          dv.labels = c("Study 1 (2.1)", "Study 2 (2.2)","Combined (2.3)", "Combined (robust samples) (2.4)", "Combined (2.5)"),
          string.est = "OR (95% CI)",
          string.stat = "z",
          string.p = "p-value",
          pred.labels = predictors.labs,
          order.terms = c(2:7,1))

```
*Note:* Predictors are centered at their grand mean (in the combined sample).  

<br>

**Figure 3: model prediction**

```{r, fig.width=6, fig.height=4}
#setting prediction of probabilities
#https://fromthebottomoftheheap.net/2017/05/01/glm-prediction-intervals-i/

ilink <- family(tmod4)$linkinv
pd.perceived.performance <- data.frame(perceived.performance.item1 = seq(1, 7, length = 100),
                 perceived.performance.item2 = seq(1, 7, length = 100),
                 perceived.performance.item3 = seq(1, 7, length = 100)) 

pd.perceived.performance <- pd.perceived.performance %>% 
  cbind(predict(tmod4, pd.perceived.performance, type = "link", se.fit = TRUE)[1:2]) %>% 
  transform(
    Fitted = ilink(fit),
    Upper = ilink(fit + (1.96 * se.fit)),
    Lower = ilink(fit - (1.96 * se.fit))
  )


ggplot(school.all %>% filter(advice == "algorithm"),
       aes(x = perceived.performance.item2, y = as.numeric(follow.advice))) +
  geom_line(data = pd.perceived.performance, aes(y = Fitted, x = perceived.performance.item2)) +
  geom_line(
    data = pd.perceived.performance,
    aes(y = Lower),
    size = 0.1,
    #color = 'gray10',
    linetype = "longdash"
  ) +
  geom_line(
    data = pd.perceived.performance,
    aes(y = Upper),
    size = 0.1,
    #color = 'gray10',
    linetype = "longdash"
  ) +
  scale_x_continuous(name = '\n"Algorithms make better judgments than humans on most tasks"', breaks = 1:7) +
  scale_y_continuous(
    name = "",
    breaks = seq(0, 0.6, 0.1),
    limits = c(0, 0.60),
    labels = scales::percent_format(accuracy = 1)
  ) +
  ggtitle("Following algorithmic advice",subtitle = "(predicted probabilities)")+
  theme(#panel.grid.major.y = element_line(colour = "grey90"),
        #panel.grid.minor.y = element_line(colour = "grey98"),
        plot.title.position = "plot")+
  geom_ribbon(aes(ymin = pd.advice.algorithm$Lower[1], ymax = pd.advice.algorithm$Upper[1], x = perceived.performance.item2),
                fill = "steelblue2", alpha = 0.2, inherit.aes = FALSE)

```
*Note:* Lines represent prediction and 95% CIs, based on Model 2.4. Ribbon represents predicted 95% CIs for human-expert condition, based on model 1.4.   

<br>

The experimental findings that we have presented thus far, which build on data from two studies, do not lend support for a general pattern of automatic adherence to algorithmic advice. Rather, they suggest that people do not tend to follow the default policy decision proposed by an algorithmic decisional aid much more than they do for an advice by a human-expert. Certainly, we cannot completely rule out the possibility that people are more likely to follow an algorithmic than a human-expert advice in our experimental setting (i.e. type II error). Yet, given our large sample from the two studies, we are able to infer with sufficient confidence that even if such an effect exists -- that averaged treatment effect is at best of a relatively small size. At the same time, our observational survey data provides evidence that at least those participants who perceive algorithms *a priori* as having greater capacities are significantly more likely to follow an algorithmic decisional aid against contradicting evidence, and we show that this effect is not confounded by age and other demographic variables. Our findings thus suggest, in line with our second hypothesis, that people with favorable views of algorithms' performative capacities are indeed more likely to over-trust algorithmic predictions (arguably more than an equivalent predictions by human-experts). 

```{r}

tmod1 <- glm(teacher.fired~moroccan+advice.algorithm,
             family = "binomial",
    data = school.s2_01.long %>% 
  filter(ILE==4))

tmod2 <- tmod1 %>% update(.~.+moroccan*advice.algorithm)

tmod3 <- tmod2 %>% update(data= school.s2_01.long %>% 
  filter(ILE==4,
         man.check.all==1) )

tmod4 <- tmod2 %>% update(.~.+
                            scale(female, scale = F) +
                            scale(age, scale = F) +
                            scale(income, scale = F))


table.study.2_1 <- school.s2_01.long %>%
  filter(ILE==4) %>% 
  group_by(moroccan) %>% 
  dplyr::summarise(n = n(),
            percent.follow.advice = (mean(teacher.fired))*100) %>% 
  data.frame()

table.study.2_2 <- school.s2_01.long %>%
  filter(ILE==4) %>% 
  group_by(moroccan,
    advice) %>% 
  dplyr::summarise(n = n(),
            percent.follow.advice = (mean(teacher.fired))*100) %>% 
  data.frame()




diff.human <- (table.study.2_2 %>% filter(advice=="human",moroccan==1) %>% select(percent.follow.advice) %>% max())-(table.study.2_2 %>% filter(advice=="human",moroccan==0) %>% select(percent.follow.advice) %>% max())

diff.algorithm <- (table.study.2_2 %>% filter(advice=="algorithm",moroccan==1) %>% select(percent.follow.advice) %>% max())-(table.study.2_2 %>% filter(advice=="algorithm",moroccan==0) %>% select(percent.follow.advice) %>% max())


```


We now turn to discuss the results of our second study in relation to our hypotheses of selective adherence. To reiterate, in study 2 we also experimentally tested whether participants were more likely to fire the teacher with the lowest ILE score, when this teacher is identified as a member of a stereotyped ethnic minority group -- in our case, Moroccan (H3). In addition, our 2x2 factorial design enables us to compare the effect of the random assignment (Moroccan/Dutch teacher) across the different types of advice (algorithmic versus human-expert), putting to rigorous experimental test the hypothesis that algorithms exacerbate this pattern of selective adherence and enhance group disparities (H4). 

**Table 3** includes the results of our regression analyses for these two later hypotheses. In Model 3.1, we regressed our outcome variable on the two manipulations, and thereafter in Model 3.2 we add their interaction. Then, similarly to our previous tables, in Models 3.3 and 3.4 we replicate our model on our robust sample, and add additional controls. Recall that in all these analyses, we limited our sample to participants of Dutch origin.   

We find a main effect for the "Moroccan teacher" manipulation, in the expected direction. A Moroccan teacher with a low ILE score is 50% [-5%--140%] more likely to be fired, compared to a Dutch teacher with the same score (Model 3.1, *p*=`r ((tmod1 %>% broom::tidy() %>% filter(term=="moroccan") %>% select(p.value) %>% max())/2) %>% round(3)`, one-sided test), and this effect survives our additional tests of controlling for covariates and filtering out those who did not properly read the task. Descriptively, `r table.study.2_1 %>% filter(moroccan==1) %>% .$percent.follow.advice %>% round(1)`% of the participants who were shown with a Moroccan teacher chose to fire her, compared with `r table.study.2_1 %>% filter(moroccan==0) %>% .$percent.follow.advice %>% round(1)`% of those who were shown with a Dutch teacher. As for the interaction between the Moroccan teacher condition and the algorithmic advice condition, it is not statistically significant in any of our interaction models (3.2--3.4). Hence, while we do find evidence that participants are more inclined to follow the ILE score's default when it matches the stereotypical view of that teacher, our findings do not suggest that this bias is increased when the score is produced by a computer algorithm. If anything, the direction of the interaction terms in our models are consistently negative, suggesting that it is actually much more likely that algorithmic scores attenuate, rather than exacerbate, group disparities. Descriptively, the differences between the Moroccan and Dutch teacher in the human-expert advice group were `r (diff.human) %>% round(1)`% (`r (table.study.2_2 %>% filter(advice=="human",moroccan==1) %>% select(percent.follow.advice) %>% max()) %>% round(1)`% versus `r (table.study.2_2 %>% filter(advice=="human",moroccan==0) %>% select(percent.follow.advice) %>% max()) %>% round(1)`%), compared with `r (diff.algorithm) %>% round(1)`% (`r (table.study.2_2 %>% filter(advice=="algorithm",moroccan==1) %>% select(percent.follow.advice) %>% max()) %>% round(1)`% versus `r (table.study.2_2 %>% filter(advice=="algorithm",moroccan==0) %>% select(percent.follow.advice) %>% max()) %>% round(1)`%) in the algorithmic group.         

  

<br>

**Table 3**

```{r}
tab_model(tmod1, tmod2, tmod3,tmod4, 
          show.stat = T,
          show.loglik = T,
          show.r2 = F,
          collapse.ci = T,
          emph.p = F,
          dv.labels = c("Study 2 (3.1)", "Study 2 (3.2)","Study 2 (robust sample) (3.3)", "Study 2 (3.4)"),
          string.est = "OR (95% CI)",
          string.stat = "z",
          string.p = "p-value",
          pred.labels = predictors.labs,
          order.terms = c(2:7,1),
          title = "Outcome variable: following the advice (binary)")

```



